{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc, ConfusionMatrixDisplay, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from format import format_table, rename_detectors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "result_path = '../../data/results'\n",
    "detectors_with_probs_numpy = [\n",
    "    'gpt2-finetuned-en3-all',\n",
    "    'electra-small-discriminator-finetuned-en3-all',\n",
    "    'electra-large-discriminator-finetuned-en3-all',\n",
    "    'bert-base-multilingual-cased-finetuned-en3-all',\n",
    "    'roberta-large-openai-detector-finetuned-en3-all',\n",
    "    'xlm-roberta-large-finetuned-en3-all',\n",
    "    'mdeberta-v3-base-finetuned-en3-all',\n",
    "    'gpt2-medium-finetuned-en3-all',\n",
    "    'mGPT-finetuned-en3-all',\n",
    "    'opt-iml-max-1.3b-finetuned-en3-all',\n",
    "    'simpleai-detector',\n",
    "    'electra-large-discriminator-finetuned-en3-gpt-3.5-turbo',\n",
    "    'electra-large-discriminator-finetuned-en3-opt-iml-max-1.3b',\n",
    "    'electra-large-discriminator-finetuned-en3-text-davinci-003',\n",
    "    'electra-large-discriminator-finetuned-en3-vicuna-13b',\n",
    "    'electra-large-discriminator-finetuned-en3-gpt-4'\n",
    "]\n",
    "\n",
    "detectors_with_probs = [\n",
    "    'roberta-large-openai-detector',\n",
    "    'grover',\n",
    "    'llmdet',\n",
    "    'zerogpt',\n",
    "    'gptzero'\n",
    "]\n",
    "\n",
    "detectors_without_probs = [\n",
    "    'gltr',\n",
    "    'longformer',\n",
    "]\n",
    "\n",
    "detectors = detectors_with_probs_numpy + detectors_with_probs + detectors_without_probs\n",
    "\n",
    "original = 'mgt_detection.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(f'{data_path}/{original}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(row):\n",
    "    if row == 'human':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "def get_index(row, df):\n",
    "    if row != row:\n",
    "        index = df.index[df['Generation'].isnull()][0]\n",
    "    else:\n",
    "        index = df.index[df['Generation'] == row][0]\n",
    "    return index\n",
    "\n",
    "\n",
    "def roc_graph(fpr, tpr, roc_auc, detector):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) Curve: {detector}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cm_graph(cm, detector):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    disp.ax_.set_title(f'Detector: {detector}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results for each detector\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for detector in detectors:\n",
    "    print(detector)\n",
    "    df = pd.read_csv(f'{result_path}/{detector}.csv')\n",
    "    # df.dropna(inplace=True)\n",
    "    df['index'] = df['Generation'].apply(lambda x: get_index(x, original_df))\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    df.sort_values(by=['index'], inplace=True)\n",
    "    df['label'] = df['pred'].apply(change_label)\n",
    "    if detector in detectors_with_probs_numpy:\n",
    "        df['probabilities'] = df['probabilities'].apply(lambda x: [float(x) for x in x[1:-1].split()])\n",
    "\n",
    "    if detector in detectors_with_probs:\n",
    "        if detector == 'gptzero':\n",
    "            df['probabilities'] = df['probabilities'].apply(lambda x: ast.literal_eval(x)[0])\n",
    "        else:\n",
    "            df['probabilities'] = df['probabilities'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    results[detector] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['true_label'] = original_df['label'].apply(change_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted, predicted_probs, detector_name, threshold=0.5, visualize=True):\n",
    "    if predicted is None:\n",
    "        predicted = [1 if x >= threshold else 0 for x in predicted_probs]\n",
    "\n",
    "\n",
    "    macro_precision = precision_score(true_labels, predicted, average='macro')\n",
    "    macro_recall = recall_score(true_labels, predicted, average='macro')\n",
    "    macro_f1 = f1_score(true_labels, predicted, average='macro')\n",
    "\n",
    "    if predicted_probs:\n",
    "        fpr, tpr, _ = roc_curve(true_labels, predicted_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        if visualize:\n",
    "            roc_graph(fpr, tpr, roc_auc, detector_name)\n",
    "            \n",
    "    data = {\n",
    "        'Macro Precision': [macro_precision],\n",
    "        'Macro Recall': [macro_recall],\n",
    "        'Macro F1-score': [macro_f1],\n",
    "        'AUC': [roc_auc] if predicted_probs else [np.nan],\n",
    "    }\n",
    "\n",
    "    if visualize:\n",
    "        cm_graph(confusion_matrix(true_labels, predicted, labels=[0, 1]), detector_name)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold(df, detectors):\n",
    "    best_thresholds = []\n",
    "    pr_best_thresholds = []\n",
    "\n",
    "    for detector in detectors:\n",
    "        fpr, tpr, thresholds = roc_curve(list(df['true_label']), list(df[detector]))\n",
    "        _, _, thresholds_pr = precision_recall_curve(list(df['true_label']), list(df[detector]))\n",
    "        f1_scores = [f1_score(list(df['true_label']), list(df[detector]) >= threshold) for threshold in thresholds_pr]\n",
    "        youdene_j = tpr - fpr\n",
    "        best_threshold_idx = np.argmax(youdene_j)\n",
    "        best_threshold = thresholds[best_threshold_idx]\n",
    "        best_thresholds.append(best_threshold)\n",
    "        pr_best_thresholds.append(thresholds_pr[np.argmax(f1_scores)])\n",
    "\n",
    "    average_best_threshold_roc = np.mean(best_thresholds)\n",
    "    average_best_threshold_pr = np.mean(pr_best_thresholds)\n",
    "    return average_best_threshold_roc, average_best_threshold_pr, best_thresholds, pr_best_thresholds      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector in detectors:\n",
    "    if detector in detectors_without_probs:\n",
    "        original_df[detector] = list(results[detector]['label'])\n",
    "    else:\n",
    "        probs = list(results[detector]['probabilities'].apply(lambda x: x[1]))\n",
    "        original_df[detector] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(df, thresholds=0.5, visualize=True):\n",
    "    final_df = pd.DataFrame()\n",
    "    list_threshold = False\n",
    "\n",
    "    if type(thresholds) == list:\n",
    "        list_threshold = True\n",
    "\n",
    "    for detector_idx, detector in enumerate(detectors):\n",
    "        threshold = 0.5\n",
    "        \n",
    "        if detector in detectors_without_probs:\n",
    "            metrics = get_metrics(list(df['true_label']), list(df[detector]), None, detector, visualize=visualize)\n",
    "        else:\n",
    "            if list_threshold:\n",
    "                threshold = thresholds[detector_idx]\n",
    "            else:\n",
    "                threshold = thresholds\n",
    "            metrics = get_metrics(list(df['true_label']), None, list(df[detector]), detector, threshold, visualize=visualize)\n",
    "\n",
    "        final_df = pd.concat([\n",
    "            final_df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    'detector': [detector],\n",
    "                    'threshold': [threshold],\n",
    "                    **metrics,\n",
    "                }\n",
    "            ).set_index('detector')    \n",
    "        ])\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold, best_threshold_pr, best_thresholds, best_thresholds_pr = get_best_threshold(original_df, detectors_with_probs_numpy + detectors_with_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(original_df, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the global best threshold based on ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df = get_results(original_df, best_thresholds, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df[['threshold', 'Macro Precision', 'Macro Recall', 'Macro F1-score', 'AUC']].round(2)\n",
    "def bootstrap_metrics(y_pred, y_true, threshold=0.5, calc_auc=True):\n",
    "    \"\"\"\n",
    "    Bootstrapping based estimate.\n",
    "\n",
    "    Return mean and confidence interval (lower and upper bound)\n",
    "    \"\"\"\n",
    "\n",
    "    auc_scores = []\n",
    "    macro_f1_scores = []\n",
    "    macro_precision_scores = []\n",
    "    macro_recall_scores = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        idx = np.random.choice(len(y_pred), len(y_pred), replace=True)\n",
    "        y_pred_sample = y_pred[idx]\n",
    "        y_true_sample = y_true[idx]\n",
    "\n",
    "        if calc_auc:\n",
    "            fpr, tpr, _ = roc_curve(y_true_sample, y_pred_sample)\n",
    "            auc_scores.append(auc(fpr, tpr))\n",
    "        macro_f1_scores.append(f1_score(y_true_sample, y_pred_sample >= threshold, average='macro'))\n",
    "        macro_precision_scores.append(precision_score(y_true_sample, y_pred_sample >= threshold, average='macro'))\n",
    "        macro_recall_scores.append(recall_score(y_true_sample, y_pred_sample >= threshold, average='macro'))\n",
    "\n",
    "    if calc_auc:\n",
    "        auc_mean = np.mean(auc_scores)\n",
    "        std_auc = np.std(auc_scores)\n",
    "        auc_scores = np.array(auc_scores)\n",
    "        auc_ci = f'{auc_mean:.3f} +- {1.96 * std_auc:.3f}'\n",
    "    else:\n",
    "        auc_ci = f'N/A'\n",
    "\n",
    "    macro_f1_mean = np.mean(macro_f1_scores)\n",
    "    macro_precision_mean = np.mean(macro_precision_scores)\n",
    "    macro_recall_mean = np.mean(macro_recall_scores)\n",
    "\n",
    "    std_macro_f1 = np.std(macro_f1_scores)\n",
    "    std_macro_precision = np.std(macro_precision_scores)\n",
    "    std_macro_recall = np.std(macro_recall_scores)\n",
    "\n",
    "    macro_f1_scores = np.array(macro_f1_scores)\n",
    "    macro_precision_scores = np.array(macro_precision_scores)\n",
    "    macro_recall_scores = np.array(macro_recall_scores)\n",
    "\n",
    "    # express 95% CI as one number with +- sign\n",
    "    macro_f1_ci = f'{macro_f1_mean:.3f} +- {1.96 * std_macro_f1:.3f}'\n",
    "    macro_precision_ci = f'{macro_precision_mean:.3f} +- {1.96 * std_macro_precision:.3f}'\n",
    "    macro_recall_ci = f'{macro_recall_mean:.3f} +- {1.96 * std_macro_recall:.3f}'\n",
    "\n",
    "    return {\n",
    "        'AUC': auc_ci,\n",
    "        'Macro F1-score': macro_f1_ci,\n",
    "        'Macro Precision': macro_precision_ci,\n",
    "        'Macro Recall': macro_recall_ci,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with CI for each metrics and for each detectors\n",
    "bootstrap_df = pd.DataFrame()\n",
    "\n",
    "for idx, detector in enumerate(detectors):\n",
    "    if detector in detectors_without_probs:\n",
    "        metrics = bootstrap_metrics(original_df[detector], original_df['true_label'], calc_auc=False)\n",
    "        bootstrap_df = pd.concat([\n",
    "            bootstrap_df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    'detector': [detector],\n",
    "                    'AUC': np.nan,\n",
    "                    'Macro F1-score': str(metrics['Macro F1-score']),\n",
    "                    'Macro Precision': str(metrics['Macro Precision']),\n",
    "                }\n",
    "            ).set_index('detector')    \n",
    "        ])\n",
    "    else:\n",
    "        metrics = bootstrap_metrics(original_df[detector], original_df['true_label'], best_thresholds[idx])\n",
    "        bootstrap_df = pd.concat([\n",
    "            bootstrap_df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    'detector': [detector],\n",
    "                    'AUC': str(metrics['AUC']),\n",
    "                    'Macro F1-score': str(metrics['Macro F1-score']),\n",
    "                    'Macro Precision': str(metrics['Macro Precision']),\n",
    "                }\n",
    "            ).set_index('detector')    \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rename_detectors(bootstrap_df).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_table(rename_detectors(roc_df[['threshold', 'Macro Precision', 'Macro Recall', 'Macro F1-score', 'AUC']]).T, axis=1, rounding=2).T.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the global best threshold based on Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(original_df, best_thresholds_pr, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'text-davinci-003',\n",
    "    'text-curie-001',\n",
    "    'text-babbage-001',\n",
    "    'falcon-40b-instruct',\n",
    "    'opt-iml-max-30b',\n",
    "    'vicuna-33b-v1.3',\n",
    "    'Llama-2-70b-chat-hf',\n",
    "    'Mistral-7B-Instruct-v0.1',\n",
    "    'gpt-4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df = pd.read_csv('../../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['model'] = ''\n",
    "original_df['brief'] = ''\n",
    "original_df['narrative'] = ''\n",
    "\n",
    "for index, row in original_df.iterrows():\n",
    "    if row['label'] == 'human':\n",
    "        original_df.at[index, 'model'] = 'human'\n",
    "        continue\n",
    "    idx = generation_df.index[generation_df['generated_text'] == row['Generation']]\n",
    "    if idx.size > 0:\n",
    "        idx = idx[0]\n",
    "        model_value = generation_df.loc[idx, 'model']\n",
    "        brief = generation_df.loc[idx, 'brief']\n",
    "        original_df.at[index, 'model'] = model_value\n",
    "        original_df.at[index, 'brief'] = brief\n",
    "        original_df.at[index, 'narrative'] = generation_df.loc[idx, 'narrative_idx']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_model(df, thresholds=0.5):\n",
    "    metrics_df = pd.DataFrame()\n",
    "    list_threshold = False\n",
    "\n",
    "    if type(thresholds) == list:\n",
    "        list_threshold = True\n",
    "    \n",
    "    for detector_idx, detector in enumerate(detectors):\n",
    "        if list_threshold:\n",
    "            if detector_idx >= len(thresholds):\n",
    "                threshold = 0.5\n",
    "            else:\n",
    "                threshold = thresholds[detector_idx]\n",
    "        else:\n",
    "            threshold = thresholds\n",
    "            \n",
    "        metrics = get_metrics(list(df['true_label']), None, list(df[detector]), detector, visualize=False, threshold=threshold)\n",
    "        metrics_df = pd.concat([\n",
    "            metrics_df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    'detector': [detector],\n",
    "                    'threshold': [threshold],\n",
    "                    **metrics,\n",
    "                }\n",
    "            ).set_index('detector')    \n",
    "        ])\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame(index=detectors, columns=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model_df = format_table(get_results_model(original_df[(original_df['model'] == model) | (original_df['model'] == 'human')]).T, rounding=2).T\n",
    "    for detector in detectors:\n",
    "        f1, roc_auc = model_df.loc[detector, 'Macro F1-score'], model_df.loc[detector, 'AUC']\n",
    "        models_df.loc[detector, model] = f'{f1} / {roc_auc}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    df.rename(columns={\n",
    "        'gpt-3.5-turbo': '\\\\textbf{ChatGPT}',\n",
    "        'text-davinci-003': '\\\\textbf{GPT-3 Davinci}',\n",
    "        'text-curie-001': '\\\\textbf{GPT-3 Curie}',\n",
    "        'text-babbage-001': '\\\\textbf{GPT-3 Babbage}',\n",
    "        'falcon-40b-instruct': '\\\\textbf{Falcon}',\n",
    "        'opt-iml-max-30b': '\\\\textbf{OPT-IML-Max}',\n",
    "        'vicuna-33b-v1.3': '\\\\textbf{Vicuna}',\n",
    "        'Llama-2-70b-chat-hf': '\\\\textbf{Llama2}',\n",
    "        'Mistral-7B-Instruct-v0.1': '\\\\textbf{Mistral}',\n",
    "        'gpt-4': '\\\\textbf{GPT-4}',\n",
    "    }, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rename_columns(rename_detectors(models_df)).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame(index=detectors, columns=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model_df = format_table(get_results_model(original_df[(original_df['model'] == model) | (original_df['model'] == 'human')], thresholds=best_thresholds).T, rounding=2).T\n",
    "\n",
    "    for detector in detectors:\n",
    "        f1, roc_auc = model_df.loc[detector, 'Macro F1-score'], model_df.loc[detector, 'AUC']\n",
    "        models_df.loc[detector, model] = f'{f1} / {roc_auc}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(' +', ' ', rename_columns(rename_detectors(models_df)).to_latex()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veraai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
