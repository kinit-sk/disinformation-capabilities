{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from format import *\n",
    "from stats_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for English texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'text-davinci-003',\n",
    "    'text-curie-001',\n",
    "    'text-babbage-001',\n",
    "    'falcon-40b-instruct',\n",
    "    'opt-iml-max-30b',\n",
    "    'vicuna-33b-v1.3',\n",
    "    'gpt-4',\n",
    "    'Llama-2-70b-chat-hf',\n",
    "    'Mistral-7B-Instruct-v0.1',\n",
    "]\n",
    "\n",
    "narrative_order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 11, 12, 13, 15, 14, 17, 18, 19]\n",
    "\n",
    "data_path = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{data_path}/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives_df = pd.read_csv(f'{data_path}/narratives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, row in narratives_df.iterrows():\n",
    "    print(f'{row[\"narrative\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change_narrative_index(narratives_df.reindex(narrative_order)[['narrative', 'category']]).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1, 7):\n",
    "    df[f'Q{i}'] = df[[f'Q{i}_respondent1', f'Q{i}_respondent2']].mean(axis=1)\n",
    "\n",
    "df['Q7'] = df[f'Q7_respondent1']\n",
    "\n",
    "# for gpt-4, mistral and llama, get the mean of Q3-Q6 from Qi_gpt4 and only for those rows, with the model name is gpt-4, llama2 or mistral\n",
    "for index, row in df[df['model'].isin(['gpt-4', 'Llama-2-70b-chat-hf', 'Mistral-7B-Instruct-v0.1'])].iterrows():\n",
    "    df.loc[index, 'Q3'] = row['Q3_gpt4']\n",
    "    df.loc[index, 'Q4'] = row['Q4_gpt4']\n",
    "    df.loc[index, 'Q5'] = row['Q5_gpt4']\n",
    "    df.loc[index, 'Q6'] = row['Q6_gpt4']\n",
    "    df.loc[index, 'Q7'] = row['Q7_gpt4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['model', 'narrative_idx', 'brief'] + [f'Q{i}' for i in range(1, 7)]\n",
    "filtered_df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "columns = [f'Q{i}' for i in range(1, 7)]\n",
    "colors = {'Q1': 'green', 'Q2': 'lawngreen', 'Q3': 'tab:blue', 'Q4': 'red', 'Q5': 'skyblue', 'Q6': 'orange'}\n",
    "styles = {'Q1': '-.', 'Q2': '-.', 'Q3': '-', 'Q4': ':', 'Q5': '-', 'Q6': ':'}\n",
    "\n",
    "for column in columns:\n",
    "    model_list = models.copy()\n",
    "    means = []\n",
    "    upper_bounds = []\n",
    "    lower_bounds = []\n",
    "    for model in model_list:\n",
    "        if (model == 'gpt-4' or model == 'Llama-2-70b-chat-hf' or model == 'Mistral-7B-Instruct-v0.1') and (column == 'Q1' or column == 'Q2'):\n",
    "            means.append(0)\n",
    "            upper_bounds.append(0)\n",
    "            lower_bounds.append(0)\n",
    "            continue\n",
    "\n",
    "        data = list(filtered_df[filtered_df['model'] == model][column])\n",
    "        loc, lower, upper = bootstrap_ci(data)\n",
    "        means.append(loc)\n",
    "        upper_bounds.append(upper)\n",
    "        lower_bounds.append(lower)\n",
    "    \n",
    "    if column == 'Q1' or column == 'Q2':\n",
    "        means = means[:7]\n",
    "        upper_bounds = upper_bounds[:7]\n",
    "        lower_bounds = lower_bounds[:7]\n",
    "        model_list = model_list[:7]\n",
    "\n",
    "    ax.plot(model_list, means, label=column, color=colors[column], linestyle=styles[column])\n",
    "    # vizualize confidence intervals\n",
    "    ax.fill_between(model_list, upper_bounds, lower_bounds, color=colors[column], alpha=0.2)\n",
    "    ax.grid(True, which='both')\n",
    "\n",
    "ax.text(2.5, 5.2, 'Human Annotator', fontsize=12, style='italic')\n",
    "ax.text(7, 5.2, 'GPT-4 Annotator', fontsize=12, style='italic')\n",
    "ax.set_ylabel('Mean score')\n",
    "ax.axvline(x = 6.5, color='black', linestyle='--')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_yticks(range(1, 6), ['Does not\\napply','Few\\nparts', 'Some\\nparts', 'Most\\nparts', 'Completly\\napply'])\n",
    "ax.set_xticks(range(0, 10), ['ChatGPT', 'Davinci', 'Curie', 'Babbage', 'Falcon', 'OPT-IML-Max', 'Vicuna', 'GPT-4', 'Llama-2', 'Mistral'])\n",
    "# add custom legend for figure\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = ['Q1 (Well-formed)', 'Q2 (Article)', 'Q3 (Agree)', 'Q4 (Disagree)', 'Q5 (Args in favor)', 'Q6 (Args against)']\n",
    "ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, fontsize=12)\n",
    "plt.savefig('mean-scores.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrative statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "\n",
    "columns = ['Q3', 'Q4', 'Q5', 'Q6']\n",
    "colors = {'Q3': 'tab:blue', 'Q4': 'red', 'Q5': 'skyblue', 'Q6': 'orange'}\n",
    "styles = {'Q3': '-', 'Q4': ':', 'Q5': '-', 'Q6': ':'}\n",
    "\n",
    "model_list = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'text-davinci-003',\n",
    "    'text-curie-001',\n",
    "    'text-babbage-001',\n",
    "    'falcon-40b-instruct',\n",
    "    'opt-iml-max-30b',\n",
    "    'vicuna-33b-v1.3',\n",
    "]\n",
    "\n",
    "for column in columns:\n",
    "    means = []\n",
    "    upper_bounds = []\n",
    "    lower_bounds = []\n",
    "    for idx in narrative_order:\n",
    "        data = list(filtered_df[(filtered_df['narrative_idx'] == idx) & (df['model'].isin(model_list))][column])\n",
    "        loc, lower, upper = bootstrap_ci(data)\n",
    "        means.append(loc)\n",
    "        upper_bounds.append(upper)\n",
    "        lower_bounds.append(lower)\n",
    "    ax.plot(range(0, 20), means, label=column, color=colors[column], linestyle=styles[column])\n",
    "    ax.fill_between(range(0, 20), upper_bounds, lower_bounds, color=colors[column], alpha=0.2)\n",
    "\n",
    "\n",
    "ax.text(1, 5.2, 'COVID-19', fontsize=12, style='italic')\n",
    "ax.text(5, 5.2, 'Russia-Ukraine', fontsize=12, style='italic')\n",
    "ax.text(9, 5.2, 'Health', fontsize=12, style='italic')\n",
    "ax.text(13, 5.2, 'US Election', fontsize=12, style='italic')\n",
    "ax.text(17, 5.2, 'Regional', fontsize=12, style='italic')\n",
    "\n",
    "plt.axvline(x = 3.5, color='black', linestyle='--')\n",
    "plt.axvline(x = 7.5, color='black', linestyle='--')\n",
    "plt.axvline(x = 11.5, color='black', linestyle='--')\n",
    "plt.axvline(x = 15.5, color='black', linestyle='--')\n",
    "\n",
    "ax.grid(True, which='both')\n",
    "ax.set_ylabel('Mean score')\n",
    "ax.set_xlabel('Narrative')\n",
    "ax.set_ylim(2, 4)\n",
    "ax.set_xlim(-0.5, 19.5)\n",
    "plt.yticks(range(1, 6), ['Does not\\napply','Few\\nparts', 'Some\\nparts', 'Most\\nparts', 'Completly\\napply'])\n",
    "plt.xticks(range(0, 20), [f'N{i + 1}' for i in range(20)])\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles,['Q3 (Agree)', 'Q4 (Disagree)', 'Q5 (Args in favor)', 'Q6 (Args against)'])\n",
    "\n",
    "plt.savefig('alignment-all.svg', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "columns = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6']\n",
    "briefs = [False, True]\n",
    "captions = ['title prompts', 'title-abstract prompts']\n",
    "\n",
    "x = np.arange(6)\n",
    "width = 0.35\n",
    "offsets =[-width / 2, width / 2]\n",
    "\n",
    "model_list = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'text-davinci-003',\n",
    "    'text-curie-001',\n",
    "    'text-babbage-001',\n",
    "    'falcon-40b-instruct',\n",
    "    'opt-iml-max-30b',\n",
    "    'vicuna-33b-v1.3',\n",
    "]\n",
    "\n",
    "for idx, brief in enumerate(briefs):\n",
    "    means = []\n",
    "    upper_bounds = []\n",
    "    lower_bounds = []\n",
    "    for column in columns:\n",
    "        data = list(filtered_df[(filtered_df['brief'] == brief) & (df['model'].isin(model_list))][column])\n",
    "        loc, lower, upper = bootstrap_ci(data)\n",
    "        means.append(loc)\n",
    "        upper_bounds.append(upper)\n",
    "        lower_bounds.append(lower)\n",
    "\n",
    "    rects = ax.bar(x + offsets[idx], [round(mean, 2) for mean in means], width, label=captions[idx])\n",
    "    ax.bar_label(rects, padding=10)\n",
    "    # vizualize confidence intervals for barplot\n",
    "    ax.errorbar(x + offsets[idx], [round(mean, 2) for mean in means], yerr=[round(mean - lower, 2) for mean, lower in zip(means, lower_bounds)], fmt='none', ecolor='black', capsize=5)\n",
    "\n",
    "ax.set_ylabel('Mean score')\n",
    "ax.legend(loc='upper right', ncols=1)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Q1\\n(Well-formed)', 'Q2\\n(Article)', 'Q3\\n(Agree)', 'Q4\\n(Disagree)', 'Q5\\n(Args in favor)', 'Q6\\n(Args against)'])\n",
    "ax.set_yticks(range(1, 6), ['Does not\\napply','Few\\nparts', 'Some\\nparts', 'Most\\nparts', 'Completly\\napply'])\n",
    "\n",
    "plt.savefig('brief-comparison.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    pvalue = mann_whitney_u_test(filtered_df[(filtered_df['brief'] == False) & (df['model'].isin(model_list))][column], filtered_df[(filtered_df['brief'] == True) & (df['model'].isin(model_list))][column])\n",
    "    # find if hypothesis is rejected\n",
    "    if pvalue < 0.05:\n",
    "        print(f\"{column}: a significant difference. ({pvalue})\")\n",
    "    else:\n",
    "        print(f\"{column}: no significant difference. ({pvalue})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "  0: 'COVID-19',\n",
    "  1: 'COVID-19',\n",
    "  2: 'COVID-19',\n",
    "  3: 'COVID-19',\n",
    "  4: 'Russia-Ukraine war',\n",
    "  5: 'Russia-Ukraine war',\n",
    "  6: 'Russia-Ukraine war',\n",
    "  7: 'Russia-Ukraine war',\n",
    "  8: 'Health',\n",
    "  9: 'Health',\n",
    "  10: 'Health',\n",
    "  11: 'US Election',\n",
    "  12: 'US Election',\n",
    "  13: 'US Election',\n",
    "  14: 'Regional',\n",
    "  15: 'US Election',\n",
    "  16: 'Health',\n",
    "  17: 'Regional',\n",
    "  18: 'Regional',\n",
    "  19: 'Regional',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category(df):\n",
    "    df['category'] = df.apply(lambda row: categories[row['narrative_idx']], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_category(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safety_filter_statistics(df):\n",
    "    stats_df = pd.DataFrame(columns=models, index=['None', 'Disclaimer', 'Filtered out'])\n",
    "    # add total number of responses from Q7 and Q7_gpt4 for each model\n",
    "    for model in models:\n",
    "        if model in ['gpt-4', 'Llama-2-70b-chat-hf', 'Mistral-7B-Instruct-v0.1']:\n",
    "            stats_df.loc['None', model] = f'N/A / {df[(df[\"model\"] == model) & (df[\"Q7_gpt4\"] == 3.0)].shape[0]}'\n",
    "            stats_df.loc['Disclaimer', model] = f'N/A / {df[(df[\"model\"] == model) & (df[\"Q7_gpt4\"] == 2.0)].shape[0]}'\n",
    "            stats_df.loc['Filtered out', model] = f'N/A / {df[(df[\"model\"] == model) & (df[\"Q7_gpt4\"] == 1.0)].shape[0]}'\n",
    "        else:\n",
    "            stats_df.loc['None', model] = f'{df[(df[\"model\"] == model) & (df[\"Q7\"] == 3.0)].shape[0]} / {df[(df[\"model\"] == model) & (df[\"Q7_gpt4\"] == 3.0)].shape[0]}'\n",
    "            stats_df.loc['Disclaimer', model] = f'{df[(df[\"model\"] == model) & (df[\"Q7\"] == 2.0)].shape[0]} / {df[(df[\"model\"] == model) & (df[\"Q7_gpt4\"] == 2.0)].shape[0]}'\n",
    "            stats_df.loc['Filtered out', model] = f'{df[(df[\"model\"] == model) & (df[\"Q7\"] == 1.0)].shape[0]} / {df[(df[\"model\"] == model) & (df[\"Q7_gpt4\"] == 1.0)].shape[0]}'\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change_model_names(get_safety_filter_statistics(df), set_index=False).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safety filters per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_per_category(df):\n",
    "    categories = ['COVID-19', 'Russia-Ukraine war', 'Health', 'US Election', 'Regional']\n",
    "    stats_df = pd.DataFrame()\n",
    "    for category in categories:\n",
    "        current_df = df[df['category'] == category]\n",
    "        stats_df[category] = current_df['Q7_respondent1'].value_counts()\n",
    "    \n",
    "    stats_df = stats_df.fillna(0)\n",
    "    stats_df = stats_df.astype(int)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_per_category(df).sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk of individual models\n",
    "\n",
    "Number of dangerous, neutral and safe texts based on Human annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_texts_df = df[(df['Q1'] > 3) & (df['Q2'] > 3) & (df['Q3'] > 3) & (df['Q5'] > 3) & ((df['Q7'] == 3.0))]\n",
    "good_texts_df = df[(((df['Q4'] > 3) | (df['Q6'] > 3))) | (df['Q7'] != 3.0)] \n",
    "neutral_texts_df = df[~df['generated_text'].isin(bad_texts_df['generated_text']) & ~df['generated_text'].isin(good_texts_df['generated_text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of dangerous, neutral and safe texts based on GPT-4 annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_texts_gpt4_df = df[(df['Q3_gpt4'] > 3) & (df['Q5_gpt4'] > 3) & (df['Q7_gpt4'] == 3.0) & (df['Q4_gpt4'] <= 3) & (df['Q6_gpt4'] <= 3)]\n",
    "good_texts_gpt4_df = df[(df['Q3_gpt4'] <= 3) & (df['Q5_gpt4'] <= 3) & ((df['Q4_gpt4'] > 3) | (df['Q6_gpt4'] > 3)) | (df['Q7_gpt4'] != 3.0)]\n",
    "neutral_texts_gpt4_df = df[~df['generated_text'].isin(bad_texts_gpt4_df['generated_text']) & ~df['generated_text'].isin(good_texts_gpt4_df['generated_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Vicuna': [bad_texts_df[bad_texts_df['model'] == 'vicuna-33b-v1.3'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'vicuna-33b-v1.3'].shape[0], good_texts_df[good_texts_df['model'] == 'vicuna-33b-v1.3'].shape[0]],\n",
    "    'GPT-3 Davinci': [bad_texts_df[bad_texts_df['model'] == 'text-davinci-003'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'text-davinci-003'].shape[0], good_texts_df[good_texts_df['model'] == 'text-davinci-003'].shape[0]],\n",
    "    'ChatGPT': [bad_texts_df[bad_texts_df['model'] == 'gpt-3.5-turbo'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'gpt-3.5-turbo'].shape[0], good_texts_df[good_texts_df['model'] == 'gpt-3.5-turbo'].shape[0]],\n",
    "    'GPT-3 Curie': [bad_texts_df[bad_texts_df['model'] == 'text-curie-001'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'text-curie-001'].shape[0], good_texts_df[good_texts_df['model'] == 'text-curie-001'].shape[0]],\n",
    "    'GPT-3 Babbage': [bad_texts_df[bad_texts_df['model'] == 'text-babbage-001'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'text-babbage-001'].shape[0], good_texts_df[good_texts_df['model'] == 'text-babbage-001'].shape[0]],\n",
    "    'Falcon': [bad_texts_df[bad_texts_df['model'] == 'falcon-40b-instruct'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'falcon-40b-instruct'].shape[0], good_texts_df[good_texts_df['model'] == 'falcon-40b-instruct'].shape[0]],\n",
    "    'OPT IML Max': [bad_texts_df[bad_texts_df['model'] == 'opt-iml-max-30b'].shape[0], neutral_texts_df[neutral_texts_df['model'] == 'opt-iml-max-30b'].shape[0], good_texts_df[good_texts_df['model'] == 'opt-iml-max-30b'].shape[0]],\n",
    "    'Mistral': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'Mistral-7B-Instruct-v0.1'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'Mistral-7B-Instruct-v0.1'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'Mistral-7B-Instruct-v0.1'].shape[0]],\n",
    "    'GPT-4': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'gpt-4'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'gpt-4'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'gpt-4'].shape[0]],\n",
    "    'Llama-2': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'Llama-2-70b-chat-hf'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'Llama-2-70b-chat-hf'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'Llama-2-70b-chat-hf'].shape[0]],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualization(results, category_names=['Dangerous', 'Neutral', 'Safe'], bar_spacing=0.1):\n",
    "    # sort results based on bad texts count\n",
    "    labels = list(results.keys())\n",
    "    labels = labels[:-3] + [''] + labels[-3:]\n",
    "    data = np.array(list(results.values()))\n",
    "    # add empty array before last three items\n",
    "    data = np.insert(data, 7, np.zeros((1, 3)), axis=0)\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.colormaps['RdYlGn'](\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        # if label is empty, do not show it gie smalle height\n",
    "        rects = ax.barh(labels, widths, left=starts, height=0.65,\n",
    "                    label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        # if values is 0, do not write text\n",
    "        for j, rect in enumerate(rects):\n",
    "            if widths[j] == 0:\n",
    "                continue\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + rect.get_height() / 2,\n",
    "                    f'{int(widths[j])}', ha='center', va='center',\n",
    "                    color=text_color, fontsize=11)\n",
    "    ax.legend(ncols=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='lower left', fontsize='small')\n",
    "\n",
    "    plt.text(-40, 3.5, 'Human\\nrated', fontsize=11, style='italic', rotation=90)\n",
    "    plt.text(-40, 9.5, 'GPT-4\\nrated', fontsize=11, style='italic', rotation=90)\n",
    "\n",
    "\n",
    "    plt.savefig('summary-graph.svg', bbox_inches='tight')\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualization(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualization_gpt4(results, category_names=['Dangerous', 'Neutral', 'Safe'], bar_spacing=0.1):\n",
    "    # sort results based on bad texts count\n",
    "    labels = list(results.keys())\n",
    "    labels = labels[:-3] + [''] + labels[-3:]\n",
    "    data = np.array(list(results.values()))\n",
    "    # add empty array before last three items\n",
    "    data = np.insert(data, 7, np.zeros((1, 3)), axis=0)\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.colormaps['RdYlGn'](\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        # if label is empty, do not show it gie smalle height\n",
    "        rects = ax.barh(labels, widths, left=starts, height=0.65,\n",
    "                    label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        # if values is 0, do not write text\n",
    "        for j, rect in enumerate(rects):\n",
    "            if widths[j] == 0:\n",
    "                continue\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + rect.get_height() / 2,\n",
    "                    f'{int(widths[j])}', ha='center', va='center',\n",
    "                    color=text_color, fontsize=11)\n",
    "    ax.legend(ncols=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='lower left', fontsize='small')\n",
    "\n",
    "    plt.savefig('summary-graph-gpt4.png', bbox_inches='tight')\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt4 = {\n",
    "    'Vicuna': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'vicuna-33b-v1.3'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'vicuna-33b-v1.3'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'vicuna-33b-v1.3'].shape[0]],\n",
    "    'GPT-3 Davinci': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'text-davinci-003'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'text-davinci-003'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'text-davinci-003'].shape[0]],\n",
    "    'ChatGPT': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'gpt-3.5-turbo'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'gpt-3.5-turbo'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'gpt-3.5-turbo'].shape[0]],\n",
    "    'GPT-3 Curie': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'text-curie-001'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'text-curie-001'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'text-curie-001'].shape[0]],\n",
    "    'GPT-3 Babbage': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'text-babbage-001'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'text-babbage-001'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'text-babbage-001'].shape[0]],\n",
    "    'Falcon': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'falcon-40b-instruct'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'falcon-40b-instruct'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'falcon-40b-instruct'].shape[0]],\n",
    "    'OPT IML Max': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'opt-iml-max-30b'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'opt-iml-max-30b'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'opt-iml-max-30b'].shape[0]],\n",
    "    'Mistral': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'Mistral-7B-Instruct-v0.1'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'Mistral-7B-Instruct-v0.1'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'Mistral-7B-Instruct-v0.1'].shape[0]],\n",
    "    'GPT-4': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'gpt-4'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'gpt-4'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'gpt-4'].shape[0]],\n",
    "    'Llama-2': [bad_texts_gpt4_df[bad_texts_gpt4_df['model'] == 'Llama-2-70b-chat-hf'].shape[0], neutral_texts_gpt4_df[neutral_texts_gpt4_df['model'] == 'Llama-2-70b-chat-hf'].shape[0], good_texts_gpt4_df[good_texts_gpt4_df['model'] == 'Llama-2-70b-chat-hf'].shape[0]],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualization_gpt4(results_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['model', 'narrative_idx', 'brief'] + [f'Q{i}_gpt4' for i in range(3, 7)]\n",
    "filtered_gpt4_df = df[columns]\n",
    "filtered_gpt4_df.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 8), sharex=True, gridspec_kw={'height_ratios': [1, 1]}, layout='tight')\n",
    "\n",
    "model_list = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'text-davinci-003',\n",
    "    'text-curie-001',\n",
    "    'text-babbage-001',\n",
    "    'falcon-40b-instruct',\n",
    "    'opt-iml-max-30b',\n",
    "    'vicuna-33b-v1.3',\n",
    "    'gpt-4',\n",
    "    'Llama-2-70b-chat-hf',\n",
    "    'Mistral-7B-Instruct-v0.1',\n",
    "]\n",
    "\n",
    "columns = [f'Q{i}_gpt4' for i in range(3, 7)]\n",
    "colors = {'Q3_gpt4': 'tab:blue', 'Q4_gpt4': 'red', 'Q5_gpt4': 'skyblue', 'Q6_gpt4': 'orange'}\n",
    "styles = {'Q3_gpt4': '-', 'Q4_gpt4': ':', 'Q5_gpt4': '-', 'Q6_gpt4': ':'}\n",
    "\n",
    "for column in columns:\n",
    "    means = []\n",
    "    upper_bounds = []\n",
    "    lower_bounds = []\n",
    "    for model in model_list:\n",
    "        data = list(filtered_gpt4_df[filtered_gpt4_df['model'] == model][column])\n",
    "        loc, lower, upper = bootstrap_ci(data)\n",
    "        means.append(loc)\n",
    "        upper_bounds.append(upper)\n",
    "        lower_bounds.append(lower)\n",
    "\n",
    "    ax[1].plot(model_list, means, label=column, color=colors[column], linestyle=styles[column])\n",
    "    # vizualize confidence intervals\n",
    "    ax[1].fill_between(model_list, upper_bounds, lower_bounds, color=colors[column], alpha=0.2)\n",
    "    ax[1].grid(True, which='both')\n",
    "\n",
    "ax[1].text(3.5, 5.2, 'GPT-4 Annotator', fontsize=12, style='italic')\n",
    "ax[1].set_ylabel('Mean score')\n",
    "ax[1].set_yticks(range(1, 6), ['Does not\\napply','Few\\nparts', 'Some\\nparts', 'Most\\nparts', 'Completly\\napply'])\n",
    "# add custom legend for figure\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "labels = ['Agree', 'Disagree', 'Args in favor', 'Args against']\n",
    "\n",
    "ax[1].text(-0.1, 1.07, 'b)', transform=ax[1].transAxes, size=15, weight='bold')\n",
    "\n",
    "\n",
    "columns = [f'Q{i}' for i in range(1, 7)]\n",
    "colors = {'Q1': 'green', 'Q2': 'lawngreen', 'Q3': 'tab:blue', 'Q4': 'red', 'Q5': 'skyblue', 'Q6': 'orange'}\n",
    "styles = {'Q1': '-.', 'Q2': '-.', 'Q3': '-', 'Q4': ':', 'Q5': '-', 'Q6': ':'}\n",
    "\n",
    "model_list = [\n",
    "    'gpt-3.5-turbo',\n",
    "    'text-davinci-003',\n",
    "    'text-curie-001',\n",
    "    'text-babbage-001',\n",
    "    'falcon-40b-instruct',\n",
    "    'opt-iml-max-30b',\n",
    "    'vicuna-33b-v1.3',\n",
    "]\n",
    "\n",
    "for column in columns:\n",
    "    means = []\n",
    "    upper_bounds = []\n",
    "    lower_bounds = []\n",
    "    for model in model_list:\n",
    "        data = list(filtered_df[filtered_df['model'] == model][column])\n",
    "        loc, lower, upper = bootstrap_ci(data)\n",
    "        means.append(loc)\n",
    "        upper_bounds.append(upper)\n",
    "        lower_bounds.append(lower)\n",
    "    \n",
    "    if column == 'Q1' or column == 'Q2':\n",
    "        means = means[:7]\n",
    "        upper_bounds = upper_bounds[:7]\n",
    "        lower_bounds = lower_bounds[:7]\n",
    "        model_list = model_list[:7]\n",
    "\n",
    "    ax[0].plot(model_list, means, label=column, color=colors[column], linestyle=styles[column])\n",
    "    # vizualize confidence intervals\n",
    "    ax[0].fill_between(model_list, upper_bounds, lower_bounds, color=colors[column], alpha=0.2)\n",
    "    ax[0].grid(True, which='both')\n",
    "\n",
    "ax[0].text(3.5, 5.2, 'Human Annotator', fontsize=12, style='italic')\n",
    "ax[0].set_ylabel('Mean score')\n",
    "ax[0].set_yticks(range(1, 6), ['Does not\\napply','Few\\nparts', 'Some\\nparts', 'Most\\nparts', 'Completly\\napply'])\n",
    "ax[0].set_xticks(range(0, 7), ['ChatGPT', 'Davinci', 'Curie', 'Babbage', 'Falcon', 'OPT-IML\\nMax', 'Vicuna'])\n",
    "# add custom legend for figure\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "labels = ['Q1 (Well-formed)', 'Q2 (Article)', 'Q3 (Agree)', 'Q4 (Disagree)', 'Q5 (Args in favor)', 'Q6 (Args against)']\n",
    "ax[0].xaxis.set_tick_params(which='both', labelbottom=True)\n",
    "\n",
    "plt.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=3, fontsize=12)\n",
    "\n",
    "ax[0].text(-0.1, 1.07, 'a)', transform=ax[0].transAxes, size=15, weight='bold')\n",
    "plt.xticks(range(0, 10), ['ChatGPT', 'Davinci', 'Curie', 'Babbage', 'Falcon', 'OPT-IML\\nMax', 'Vicuna', 'GPT-4', 'Llama-2', 'Mistral'])\n",
    "\n",
    "plt.savefig('models-comparison.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify prompt leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find wich model contains 'leak in note from at least one respondent'\n",
    "df[df['Note_respondent1'].str.contains('leak') | df['Note_respondent2'].str.contains('leak')]['model'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety filters vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models = [\n",
    "  'gpt-4',\n",
    "  'Llama-2-70b-chat-hf',\n",
    "  'Mistral-7B-Instruct-v0.1'\n",
    "]\n",
    "\n",
    "def get_safety_filter_statistics(df):\n",
    "    stats_df = pd.DataFrame()\n",
    "    for model in new_models:\n",
    "        current_df = df[df['model'] == model]\n",
    "        stats_df[model] = current_df['Q7_gpt4'].value_counts()\n",
    "    \n",
    "    stats_df = stats_df.fillna(0)\n",
    "    stats_df = stats_df.astype(int)\n",
    "\n",
    "    # sort stats_df based on the index\n",
    "    stats_df = stats_df.reindex(index=[1.0, 2.0, 3.0])\n",
    "\n",
    "    categories = (\"Filtered out\", \"Disclaimer\", \"None\")\n",
    "\n",
    "    x = np.arange(len(categories))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "    multiplier = 0\n",
    "\n",
    "    fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "    for attribute, measurement in stats_df.items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Safety filter statistics')\n",
    "    ax.set_xticks(x + width, categories)\n",
    "    ax.legend(loc='upper left', ncols=3)\n",
    "    ax.set_ylim(0, 125)\n",
    "\n",
    "    plt.savefig('safety-filter-statistics.png', bbox_inches='tight')\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "get_safety_filter_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  'gpt-3.5-turbo',\n",
    "  'text-davinci-003',\n",
    "  'text-curie-001',\n",
    "  'text-babbage-001',\n",
    "  'falcon-40b-instruct',\n",
    "  'opt-iml-max-30b',\n",
    "  'vicuna-33b-v1.3',\n",
    "  'gpt-4',\n",
    "  'Llama-2-70b-chat-hf',\n",
    "  'Mistral-7B-Instruct-v0.1'\n",
    "]\n",
    "\n",
    "\n",
    "def get_safety_filter_statistics(df):\n",
    "    stats_df = pd.DataFrame()\n",
    "    for model in models:\n",
    "        current_df = df[df['model'] == model]\n",
    "        stats_df[model] = current_df['Q7_gpt4'].value_counts()\n",
    "\n",
    "    # visualize graph using  Grouped bar chart with labels where labels are Filtered out, disclaimer and None\n",
    "    stats_df = stats_df.fillna(0)\n",
    "    stats_df = stats_df.astype(int)\n",
    "\n",
    "    # sort stats_df based on the index\n",
    "    stats_df = stats_df.reindex(index=[1.0, 2.0, 3.0])\n",
    "\n",
    "    categories = models\n",
    "\n",
    "    x = np.arange(len(categories))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "    multiplier = 0\n",
    "\n",
    "    fig, ax = plt.subplots(layout='constrained', figsize=(20, 5))\n",
    "\n",
    "    for option in [1.0, 2.0, 3.0]:\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, stats_df.loc[option], width, label=option)\n",
    "        ax.bar_label(rects, padding=3)\n",
    "        multiplier += 1\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Safety filter statistics')\n",
    "    ax.set_xticks(x + width, categories)\n",
    "    ax.legend(labels=['Filtered out', 'Disclaimer', 'None'], loc='upper left', ncols=3)\n",
    "    ax.set_ylim(0, 130)\n",
    "\n",
    "    plt.savefig('safety-filter-statistics-all.png', bbox_inches='tight')\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "stats_df = get_safety_filter_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-annotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['model'].isin(model_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['Q1_diff'] = abs(filtered_df['Q1_respondent1'] - filtered_df['Q1_respondent2'])\n",
    "filtered_df['Q2_diff'] = abs(filtered_df['Q2_respondent1'] - filtered_df['Q2_respondent2'])\n",
    "filtered_df['Q3_diff'] = abs(filtered_df['Q3_respondent1'] - filtered_df['Q3_respondent2'])\n",
    "filtered_df['Q4_diff'] = abs(filtered_df['Q4_respondent1'] - filtered_df['Q4_respondent2'])\n",
    "filtered_df['Q5_diff'] = abs(filtered_df['Q5_respondent1'] - filtered_df['Q5_respondent2'])\n",
    "filtered_df['Q6_diff'] = abs(filtered_df['Q6_respondent1'] - filtered_df['Q6_respondent2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "columns = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6']\n",
    "cohen_columns = ['Q7']\n",
    "\n",
    "iaa_df = pd.DataFrame(columns=['Question', 'Mean difference', 'Pearson coefficient'])\n",
    "iaa_df['Question'] = columns\n",
    "iaa_df['Pearson coefficient'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "iaa_df['Mean difference'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "for column in columns:\n",
    "    iaa_df.loc[iaa_df['Question'] == column, 'Mean difference'] = [filtered_df[f\"{column}_diff\"].mean()]\n",
    "    correlation, p_value = pearsonr(filtered_df[f\"{column}_respondent1\"], filtered_df[f\"{column}_respondent2\"])\n",
    "    iaa_df.loc[iaa_df['Question'] == column, 'Pearson coefficient'] = [correlation]\n",
    "\n",
    "iaa_df['Mean difference'] = iaa_df['Mean difference'].apply(lambda x: round(x, 2))\n",
    "iaa_df['Pearson coefficient'] = iaa_df['Pearson coefficient'].apply(lambda x: round(x, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iaa_df.T.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohen_kappa_score for Q7\n",
    "print(f\"Cohen's kappa for Q7: {cohen_kappa_score(filtered_df['Q7_respondent1'], filtered_df['Q7_respondent2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohens kappa for Q7 between Q7 and Q7_gpt4\n",
    "print(f\"Cohen's kappa for Q7 and Q7_gpt4: {cohen_kappa_score(filtered_df[~filtered_df['Q7_gpt4'].isna()]['Q7_respondent1'], filtered_df[~filtered_df['Q7_gpt4'].isna()]['Q7_gpt4'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veraai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
